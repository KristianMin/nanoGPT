{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from datasets import load_dataset # huggingface datasets\n",
    "import glob\n",
    "\n",
    "# number of workers in .map() call\n",
    "# good number to use is ~order number of cpu cores // 2\n",
    "num_proc = 8\n",
    "\n",
    "# number of workers in load_dataset() call\n",
    "# best number might be different from num_proc above as it also depends on NW speed.\n",
    "# it is better than 1 usually though\n",
    "num_proc_load_dataset = num_proc\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "model = \"4o\"\n",
    "qa_r = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c712c844ba44709a5eeb23bad822cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/13.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting num_proc from 8 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c71356d2684af3b4102279981cc0ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/600000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 480000\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 120000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"KrisMinchev/LLMCollapseBios\", f\"data_{model}\", num_proc=num_proc_load_dataset)\n",
    "qa = load_dataset(\"KrisMinchev/LLMCollapseBios\", f\"qa_{model}\", num_proc=num_proc_load_dataset)\n",
    "split_qa = qa[\"train\"].train_test_split(test_size=qa_r, seed=2357, shuffle=True)\n",
    "split_qa[\"val\"] = split_qa.pop(\"test\")\n",
    "split_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the birth date of Lupita Troy Mendoza?\n",
      "Answer: August 11, 1936.\n"
     ]
    }
   ],
   "source": [
    "print(split_qa[\"val\"][0]['question'])\n",
    "print(split_qa[\"val\"][0]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['name', 'college', 'degree', 'birthplace', 'location', 'company', 'birthday', 'prompt', 'ids', 'len'],\n",
      "        num_rows: 80000\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['name', 'college', 'degree', 'birthplace', 'location', 'company', 'birthday', 'prompt', 'ids', 'len'],\n",
      "        num_rows: 20000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def process_bio(example):\n",
    "    ids = enc.encode_ordinary(example['output'])\n",
    "    ids.append(enc.eot_token)\n",
    "    out = {'ids': ids, 'len': len(ids)}\n",
    "    return out\n",
    "\n",
    "tokenized_bio = dataset.map(\n",
    "    process_bio,\n",
    "    remove_columns=['output'],\n",
    "    desc=\"tokenizing the splits\",\n",
    "    num_proc=num_proc,\n",
    ")\n",
    "\n",
    "def process_qa(example):\n",
    "    ids = enc.encode_ordinary(example['question'] + example['answer'])\n",
    "    ids.append(enc.eot_token)\n",
    "    out = {'ids': ids, 'len': len(ids)}\n",
    "    return out\n",
    "\n",
    "tokenized_qa = split_qa.map(\n",
    "    process_qa,\n",
    "    desc=\"tokenizing the splits\",\n",
    "    num_proc=num_proc,\n",
    ")\n",
    "print(tokenized_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "writing ./train_bio.bin: 100%|██████████| 1024/1024 [00:01<00:00, 833.77it/s]\n",
      "writing ./train_qa.bin: 100%|██████████| 1024/1024 [00:01<00:00, 904.37it/s]\n",
      "writing ./val_qa.bin: 100%|██████████| 1024/1024 [00:01<00:00, 1022.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# print(tokenized_bio.items())\n",
    "# print(tokenized_qa.items())\n",
    "for split, dset in tokenized_bio.items():\n",
    "    arr_len = np.sum(dset['len'], dtype=np.uint64)\n",
    "    filename = f'./{split}_bio.bin'\n",
    "    dtype = np.uint16 # (can do since enc.max_token_value == 50256 is < 2**16)\n",
    "    arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
    "    total_batches = 1024\n",
    "\n",
    "    idx = 0\n",
    "    for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n",
    "        # Batch together samples for faster write\n",
    "        batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
    "        arr_batch = np.concatenate(batch['ids'])\n",
    "        # Write into mmap\n",
    "        arr[idx : idx + len(arr_batch)] = arr_batch\n",
    "        idx += len(arr_batch)\n",
    "    arr.flush()\n",
    "\n",
    "for split, dset in tokenized_qa.items():\n",
    "    arr_len = np.sum(dset['len'], dtype=np.uint64)\n",
    "    filename = f'./{split}_qa.bin'\n",
    "    dtype = np.uint16 # (can do since enc.max_token_value == 50256 is < 2**16)\n",
    "    arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
    "    total_batches = 1024\n",
    "\n",
    "    idx = 0\n",
    "    for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n",
    "        # Batch together samples for faster write\n",
    "        batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
    "        arr_batch = np.concatenate(batch['ids'])\n",
    "        # Write into mmap\n",
    "        arr[idx : idx + len(arr_batch)] = arr_batch\n",
    "        idx += len(arr_batch)\n",
    "    arr.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['train_bio.bin', 'train_qa.bin']\n",
    "out_data = b''\n",
    "for fn in files:\n",
    "  with open(fn, 'rb') as fp:\n",
    "    out_data += fp.read()\n",
    "with open('train.bin', 'wb') as fp:\n",
    "  fp.write(out_data)\n",
    "\n",
    "os.rename('val_qa.bin', 'val.bin')\n",
    "# os.remove('train_*.bin')\n",
    "for f in glob.glob('train_*.bin'):\n",
    "    os.remove(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "collapse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
